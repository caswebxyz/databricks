{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d94e9e-22ad-4653-ab0c-690828d7c95b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import col, expr, round\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def fix_column_names(df):\n",
    "    for col in df.columns:\n",
    "        new_col = re.sub(r\"[ ,;{}()\\n\\t~]\", '_', col)\n",
    "        df = df.withColumnRenamed(col, new_col)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060c1aec-5401-4e96-ad2d-343549c03424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,from_json\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, TimestampType, FloatType, LongType, BooleanType, MapType\n",
    "\n",
    "\n",
    "def flatten_json(df):\n",
    "    \"\"\"\n",
    "    Flattens a pyspark dataframe containing nested JSON Structure\n",
    "\n",
    "    Args:\n",
    "     - df ( DataFrame): A pyspark dataframe containing nested JSON Structure\n",
    "    \n",
    "    Returns\n",
    "     - DataFrame: A flattened pyspark dataframe with parsed JSON Structure\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Define Schema for the stats JSON Field\n",
    "\n",
    "    stats_schema = StructType([\n",
    "        StructField(\"numRecords\", IntegerType(), True),\n",
    "        StructField(\"minValues\", MapType(StringType(), StringType()), True),\n",
    "        StructField(\"maxValues\", MapType(StringType(), StringType()), True),\n",
    "        StructField(\"nullCount\", MapType(StringType(), IntegerType()), True),\n",
    "        StructField('tightBounds', BooleanType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f7f1aa-3863-422b-8aca-9ca38bd2a006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables/\")\n",
    "##dbutils.fs.rm(\"dbfs:/FileStore/tables/\",recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d97b9534-9a9f-42c5-9877-aaa8c83d30ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#                                                         Z Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "243943de-1466-4826-8136-c0eddf0043e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the path for the delta table partitions\n",
    "delta_path_partitions = \"dbfs:/FileStore/tables/Performance/data_skipping/Zorder\"\n",
    "\n",
    "# Read csv files from the specified path into the data frame\n",
    "df = spark.read.format(\"csv\").option(\"header\",True).load(\"/FileStore/tables/sample_files_testing/*\")\n",
    "\n",
    "# Fix DataFrame Column Names\n",
    "df = fix_column_names(df)\n",
    "\n",
    "#print(\"Number of Partitions\",df.rdd.getNumPartitions())\n",
    "\n",
    "# Write the DataFrame to a DeltaTable partitioned by the country column\n",
    "df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"Country\").save(delta_path_partitions)   \n",
    "\n",
    "# Load the DeltaTable from the specified path\n",
    "delta_table_with_partitions = DeltaTable.forPath(spark, delta_path_partitions)\n",
    "\n",
    "# Display the Delta Table\n",
    "display(delta_table_with_partitions)\n",
    "\n",
    "# Read and Display the CheckSum Files from the Delta Log\n",
    "df = spark.read.json(f\"{delta_path_partitions}/_delta_log/*.json\")\n",
    "\n",
    "# Call the flattened json function\n",
    "delta_table_with_partitions_checksum = spark.read.text(f\"{delta_path_partitions}/_delta_log/*.crc\")\n",
    "display(delta_table_with_partitions_checksum)\n",
    "\n",
    "# Read and Display the JSON log file from the Delta Log\n",
    "delta_table_with_partitions_json_temp = spark.read.json(f\"{delta_path_partitions}/_delta_log/*.json\")\n",
    "\n",
    "display(delta_table_with_partitions_json_temp)\n",
    "\n",
    "delta_table_with_partitions_json = flatten_json(delta_table_with_partitions_json_temp)\n",
    "\n",
    "history_df = delta_table_with_partitions.history()\n",
    "display(history_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e86ffec-c4f4-4314-9655-3becf2be4a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the path for the delta table partitions\n",
    "delta_path_partitions = \"dbfs:/FileStore/tables/Performance/data_skipping/Zorder\"\n",
    "\n",
    "# Read csv files from the specified path into the data frame\n",
    "df = spark.read.format(\"csv\").option(\"header\",True).load(\"/FileStore/tables/sample_files_testing_2/*\")\n",
    "\n",
    "# Fix DataFrame Column Names\n",
    "df = fix_column_names(df)\n",
    "\n",
    "#print(\"Number of Partitions\",df.rdd.getNumPartitions())\n",
    "\n",
    "# Write the DataFrame to a DeltaTable partitioned by the country column\n",
    "df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"Country\").save(delta_path_partitions)   \n",
    "\n",
    "# Load the DeltaTable from the specified path\n",
    "delta_table_with_partitions = DeltaTable.forPath(spark, delta_path_partitions)\n",
    "\n",
    "# Display the Delta Table\n",
    "display(delta_table_with_partitions)\n",
    "\n",
    "# Read and Display the CheckSum Files from the Delta Log\n",
    "df = spark.read.json(f\"{delta_path_partitions}/_delta_log/*.json\")\n",
    "\n",
    "# Call the flattened json function\n",
    "delta_table_with_partitions_checksum = spark.read.text(f\"{delta_path_partitions}/_delta_log/*.crc\")\n",
    "display(delta_table_with_partitions_checksum)\n",
    "\n",
    "# Read and Display the JSON log file from the Delta Log\n",
    "delta_table_with_partitions_json_temp = spark.read.json(f\"{delta_path_partitions}/_delta_log/*.json\")\n",
    "\n",
    "display(delta_table_with_partitions_json_temp)\n",
    "\n",
    "delta_table_with_partitions_json = flatten_json(delta_table_with_partitions_json_temp)\n",
    "\n",
    "history_df = delta_table_with_partitions.history()\n",
    "display(history_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0f174e0-b826-4620-a8ba-6572ddecf5cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_table_with_partitions.optimize().executeZOrderBy([\"industry\"])\n",
    "delta_table_with_partitions_checksum = spark.read.text(f\"{delta_path_partitions}/_delta_log/*.crc\")\n",
    "display(delta_table_with_partitions_checksum)\n",
    "\n",
    "# Read and Display the JSON log file from the Delta Log\n",
    "delta_table_with_partitions_json_temp = spark.read.json(f\"{delta_path_partitions}/_delta_log/*.json\")\n",
    "delta_table_with_partitions_json = flatten_json(delta_table_with_partitions_json_temp)\n",
    "display(delta_table_with_partitions_json)\n",
    "\n",
    "history_df = delta_table_with_partitions.history()\n",
    "display(history_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8eaf70a-aa4f-4d89-ad36-ce1ae4a904bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### # Liquid Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "811c5d5f-dfe8-43b3-90fc-fb104c2fbeff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the path for the delta table partitions\n",
    "delta_path_partitions = \"dbfs:/FileStore/tables/Performance/data_skipping/clusterBy1\"\n",
    "\n",
    "# Read csv files from the specified path into the data frame\n",
    "df = spark.read.format(\"csv\").option(\"header\",True).load(\"/FileStore/tables/sample_files_testing/*\")\n",
    "\n",
    "# Fix DataFrame Column Names\n",
    "df = fix_column_names(df)\n",
    "\n",
    "#print(\"Number of Partitions\",df.rdd.getNumPartitions())\n",
    "\n",
    "# Write the DataFrame to a DeltaTable partitioned by the country column\n",
    "df.write.format(\"delta\").mode(\"overwrite\").clusterBy(\"Country\").save(delta_path_partitions)   \n",
    "\n",
    "# Load the DeltaTable from the specified path\n",
    "delta_table_with_partitions = DeltaTable.forPath(spark, delta_path_partitions)\n",
    "\n",
    "# Display the Delta Table\n",
    "display(delta_table_with_partitions)\n",
    "\n",
    "# Read and Display the CheckSum Files from the Delta Log\n",
    "df = spark.read.json(f\"{delta_path_partitions}/_delta_log/*.json\")\n",
    "\n",
    "# Call the flattened json function\n",
    "delta_table_with_partitions_checksum = spark.read.text(f\"{delta_path_partitions}/_delta_log/*.crc\")\n",
    "display(delta_table_with_partitions_checksum)\n",
    "\n",
    "# Read and Display the JSON log file from the Delta Log\n",
    "delta_table_with_partitions_json_temp = spark.read.json(f\"{delta_path_partitions}/_delta_log/*.json\")\n",
    "\n",
    "display(delta_table_with_partitions_json_temp)\n",
    "\n",
    "delta_table_with_partitions_json = flatten_json(delta_table_with_partitions_json_temp)\n",
    "\n",
    "history_df = delta_table_with_partitions.history()\n",
    "display(history_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6076dc8d-ac17-4c36-b906-7a5ed506ba91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the path for the delta table partitions\n",
    "delta_path_partitions = \"dbfs:/FileStore/tables/Performance/data_skipping/clusterBy2\"\n",
    "\n",
    "# Read csv files from the specified path into the data frame\n",
    "df = spark.read.format(\"csv\").option(\"header\",True).load(\"/FileStore/tables/sample_files_testing_2/*\")\n",
    "\n",
    "# Fix DataFrame Column Names\n",
    "df = fix_column_names(df)\n",
    "\n",
    "#print(\"Number of Partitions\",df.rdd.getNumPartitions())\n",
    "\n",
    "# Write the DataFrame to a DeltaTable partitioned by the country column\n",
    "df.write.format(\"delta\").mode(\"overwrite\").clusterBy(\"Country\").save(delta_path_partitions)   \n",
    "\n",
    "# Load the DeltaTable from the specified path\n",
    "delta_table_with_partitions = DeltaTable.forPath(spark, delta_path_partitions)\n",
    "\n",
    "# Display the Delta Table\n",
    "display(delta_table_with_partitions)\n",
    "\n",
    "# Read and Display the CheckSum Files from the Delta Log\n",
    "df = spark.read.json(f\"{delta_path_partitions}/_delta_log/*.json\")\n",
    "\n",
    "# Call the flattened json function\n",
    "delta_table_with_partitions_checksum = spark.read.text(f\"{delta_path_partitions}/_delta_log/*.crc\")\n",
    "display(delta_table_with_partitions_checksum)\n",
    "\n",
    "# Read and Display the JSON log file from the Delta Log\n",
    "delta_table_with_partitions_json_temp = spark.read.json(f\"{delta_path_partitions}/_delta_log/*.json\")\n",
    "\n",
    "display(delta_table_with_partitions_json_temp)\n",
    "\n",
    "delta_table_with_partitions_json = flatten_json(delta_table_with_partitions_json_temp)\n",
    "\n",
    "history_df = delta_table_with_partitions.history()\n",
    "display(history_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7392d36-6f62-476f-8b2c-efd0ab816c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_table_with_partitions.optimize().executeCompaction()\n",
    "history_df = delta_table_with_partitions.history()\n",
    "display(history_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "final liquid clustering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
