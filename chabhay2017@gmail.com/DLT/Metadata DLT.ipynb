{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb92387a-2fef-48a9-a696-8ef68643fe9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col,expr,lit\n",
    "from pyspark.sql.types import StructType, StringType, StructField, IntegerType, DateType, DoubleType\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4e2d04-ffad-4a95-8b79-a3fc1d41b60b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def generate_schema(input_df):\n",
    "    \"\"\"\n",
    "    Generate a pyspark schema based on specified data type mapping\n",
    "\n",
    "    Parameters:\n",
    "     - input_df: PySpark DataFrame containing columns \"TargetDataType,\" \"ColumnName,\" and \"ColumnOrder.\"\n",
    "    Returns:\n",
    "        -schema: PySpark StructType Schema.\n",
    "\n",
    "    \"\"\"\n",
    "    # Define the data type mapping\n",
    "    data_type_mapping ={\n",
    "        \"StringType\":StringType(),\n",
    "        \"IntegerType\":IntegerType(),\n",
    "        \"DoubleType\":DoubleType(),\n",
    "        \"DateType\":DateType(),\n",
    "        \"datetime\":TimestampType(),\n",
    "    }\n",
    "\n",
    "    # Collect Distinct Values of \"TargetDataType\", \" ColumnName,\" and \"ColumnOrder\"\n",
    "    distinct_datatypes = (\n",
    "        input_df.select(\"columndatatype\",\"columnname\",\"columnorder\").distinct().collect()\n",
    "    )\n",
    "\n",
    "    # Sort Distinct Datatypes based on \"ColumnOrder\"\n",
    "    distinct_datatypes = sorted(distinct_datatypes, key=lambda x: x.columnorder)\n",
    "\n",
    "    # Create Schema Fields\n",
    "    schema_fields = [\n",
    "        StructField(row.columnname, data_type_mapping[row.columndatatype], True)\n",
    "        for row in distinct_datatypes\n",
    "    ]\n",
    "\n",
    "    # Create and Return the Schema\n",
    "    schema = StructType(schema_fields)\n",
    "    return schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "057774be-1b37-4861-ad81-bd5eb66143de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def dlt_ingestion_metadata_function(row:Row, schema: StructType):\n",
    "\n",
    "    table_name = row['sourcetablename']\n",
    "    checks = row['sourcedataquality']\n",
    "    checks = json.loads(checks)\n",
    "    keys = [\"circuitId\"]\n",
    "    sequence_by = row['sequenceby']\n",
    "    file_path = row['sourcefilepath']\n",
    "    cloud_file_options = eval(row['sourcefileoptions'])\n",
    "    dq_rules = \"({0})\".format(\" and \".join(checks.values()))\n",
    "\n",
    "    @dlt.table(\n",
    "        name=\"brz_load_\"+table_name\n",
    "    )\n",
    "\n",
    "    def bronze_load():\n",
    "        df = spark.read.format(\"csv\").options(**cloud_file_options).schema(schema).load(file_path)\n",
    "        df = df.withColumn(\"file_processed_date\",F.date_format(F.current_timestamp(),\"yyyy-MM-dd HH:mm:ss\"))\n",
    "        return df\n",
    "\n",
    "    @dlt.table(\n",
    "        name = \"stag_silver_load_\"+table_name,\n",
    "    )\n",
    "\n",
    "    @dlt.expect_all(checks)\n",
    "\n",
    "    def stag_silver_table():\n",
    "        df = dlt.readStream(\"brz_load_\"+table_name)\n",
    "        df = df.withColumn(\"dq_check\",F.expr(dq_rules)).filter(\"dq_check=true\")\n",
    "        return df\n",
    "    \n",
    "    dlt.create_streaming_table(\n",
    "        name=\"silver_load_\"+table_name\n",
    "    )\n",
    "\n",
    "    dlt.apply_changes(\n",
    "        target=\"silver_load_\"+table_name,\n",
    "        source=\"stag_silver_load_\"+table_name,\n",
    "        keys=keys,\n",
    "        stored_as_scd_type=\"1\",\n",
    "        sequence_by=sequence_by\n",
    "    )\n",
    "    @dlt.table(\n",
    "        name=\"err_silver_load_\"+table_name,\n",
    "    )\n",
    "    @dlt.expect_all(checks)\n",
    "    def err_silver_load_circuit():\n",
    "        df=dlt.readStream(\"brz_load_\"+table_name)\n",
    "        df=df.withColumn(\"dq_check\",F.expr(dq_rules)).filter(\"dq_check=false\")\n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd791950-59f5-468f-b0eb-743407d5613c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"SELECT * from `poc`.`sourcemetadata` where sourcemetadaid = 3\"\n",
    "df = spark.sql(query)\n",
    "for row in df.collect():\n",
    "\n",
    "    #print(row['sourcemetadaID'])\n",
    "    schema_query = f\"(select * from `poc`.`sourceschemaconfig` where sourcemetadataid = {row['sourcemetadaID']})\"\n",
    "    df_schema = spark.sql(schema_query)\n",
    "    print(df_schema)\n",
    "    schema=generate_schema(df_schema)\n",
    "    dlt_ingestion_metadata_function(row=row,schema=schema)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Metadata DLT",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
